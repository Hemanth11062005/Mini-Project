{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2430 images belonging to 9 classes.\n",
      "Found 270 images belonging to 9 classes.\n",
      "Found 450 images belonging to 9 classes.\n",
      "WARNING:tensorflow:From c:\\Hemanth\\Mini Project\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:216: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Hemanth\\Mini Project\\venv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 1s/step - accuracy: 0.1134 - loss: 2.2269 - val_accuracy: 0.1852 - val_loss: 2.1696 - learning_rate: 1.0000e-04\n",
      "Epoch 2/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m563s\u001b[0m 7s/step - accuracy: 0.1595 - loss: 2.1696 - val_accuracy: 0.1815 - val_loss: 2.1363 - learning_rate: 1.0000e-04\n",
      "Epoch 3/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m431s\u001b[0m 6s/step - accuracy: 0.1845 - loss: 2.1333 - val_accuracy: 0.2074 - val_loss: 2.0700 - learning_rate: 1.0000e-04\n",
      "Epoch 4/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 2s/step - accuracy: 0.2314 - loss: 2.0071 - val_accuracy: 0.2000 - val_loss: 1.9520 - learning_rate: 1.0000e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 1s/step - accuracy: 0.2910 - loss: 1.9119 - val_accuracy: 0.3037 - val_loss: 1.8046 - learning_rate: 1.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 3s/step - accuracy: 0.3636 - loss: 1.7422 - val_accuracy: 0.3259 - val_loss: 1.7629 - learning_rate: 1.0000e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 1s/step - accuracy: 0.4273 - loss: 1.5562 - val_accuracy: 0.3370 - val_loss: 1.6831 - learning_rate: 1.0000e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 1s/step - accuracy: 0.5782 - loss: 1.2654 - val_accuracy: 0.2630 - val_loss: 1.9260 - learning_rate: 1.0000e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 1s/step - accuracy: 0.6941 - loss: 0.9082 - val_accuracy: 0.4519 - val_loss: 2.0582 - learning_rate: 1.0000e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 1s/step - accuracy: 0.8212 - loss: 0.5694 - val_accuracy: 0.5111 - val_loss: 2.2518 - learning_rate: 1.0000e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 1s/step - accuracy: 0.8926 - loss: 0.3826 - val_accuracy: 0.4741 - val_loss: 2.7826 - learning_rate: 1.0000e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 1s/step - accuracy: 0.9320 - loss: 0.2308 - val_accuracy: 0.4111 - val_loss: 3.0767 - learning_rate: 1.0000e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 1s/step - accuracy: 0.9788 - loss: 0.1003 - val_accuracy: 0.4963 - val_loss: 3.1582 - learning_rate: 5.0000e-05\n",
      "Epoch 14/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 1s/step - accuracy: 0.9938 - loss: 0.0384 - val_accuracy: 0.5074 - val_loss: 3.2772 - learning_rate: 5.0000e-05\n",
      "Epoch 15/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 1s/step - accuracy: 0.9965 - loss: 0.0227 - val_accuracy: 0.4815 - val_loss: 3.3504 - learning_rate: 5.0000e-05\n",
      "Epoch 16/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1187s\u001b[0m 16s/step - accuracy: 0.9980 - loss: 0.0175 - val_accuracy: 0.5074 - val_loss: 3.4051 - learning_rate: 5.0000e-05\n",
      "Epoch 17/30\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 1s/step - accuracy: 0.9970 - loss: 0.0209 - val_accuracy: 0.5074 - val_loss: 3.6019 - learning_rate: 5.0000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 446ms/step - accuracy: 0.5484 - loss: 1.3485\n",
      "Test Accuracy: 0.5222\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 650ms/step\n",
      "Predicted: beetle (0.4742)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "class PatchExtractor(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(PatchExtractor, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "    \n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\"\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def build_vit_model(input_shape, patch_size, num_patches, projection_dim, \n",
    "                     transformer_layers, num_heads, transformer_units, \n",
    "                     mlp_head_units, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    patches = PatchExtractor(patch_size)(inputs)\n",
    "    patch_projection = layers.Dense(projection_dim)(patches)\n",
    "    \n",
    "    positions = tf.range(start=0, limit=num_patches, delta=1)\n",
    "    position_embedding = layers.Embedding(input_dim=num_patches, output_dim=projection_dim)(positions)\n",
    "    encoded_patches = patch_projection + position_embedding\n",
    "    \n",
    "    for _ in range(transformer_layers):\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim, dropout=0.1)(x1, x1)\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "    \n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.GlobalAveragePooling1D()(representation)\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.3)\n",
    "    logits = layers.Dense(num_classes, activation=\"softmax\")(features)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=logits)\n",
    "\n",
    "def train_vit(training, validing, testing, num_classes, epochs=30):\n",
    "    vit_model = build_vit_model((224, 224, 3), 16, 196, 128, 6, 8, [256, 128], [512, 256], num_classes)\n",
    "    vit_model.compile(optimizer=Adam(learning_rate=1e-4), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6)\n",
    "    \n",
    "    history = vit_model.fit(training, epochs=epochs, validation_data=validing, callbacks=[early_stopping, reduce_lr])\n",
    "    vit_model.save(\"vision_transformer_model.h5\")\n",
    "    \n",
    "    test_loss, test_accuracy = vit_model.evaluate(testing)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    return vit_model, history\n",
    "\n",
    "def predict_vit(model, img_path, class_labels):\n",
    "    if not os.path.exists(img_path):\n",
    "        print(f\"Image {img_path} not found.\")\n",
    "        return\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = np.expand_dims(image.img_to_array(img) / 255.0, axis=0)\n",
    "    prediction = model.predict(img_array)\n",
    "    predicted_class = np.argmax(prediction, axis=1)[0]\n",
    "    print(f\"Predicted: {class_labels[predicted_class]} ({prediction[0][predicted_class]:.4f})\")\n",
    "\n",
    "def main():\n",
    "    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, validation_split=0.1)\n",
    "    train_dir = 'Dataset/pest/train'\n",
    "    test_dir = 'Dataset/pest/test'\n",
    "    training = train_datagen.flow_from_directory(train_dir, batch_size=32, target_size=(224, 224), subset=\"training\")\n",
    "    validing = train_datagen.flow_from_directory(train_dir, batch_size=32, target_size=(224, 224), subset='validation')\n",
    "    testing = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255).flow_from_directory(test_dir, batch_size=32, target_size=(224, 224))\n",
    "    \n",
    "    num_classes = len(training.class_indices)\n",
    "    vit_model, history = train_vit(training, validing, testing, num_classes)\n",
    "    img_test_path = 'Dataset/pest/test/beetle/jpg_33.jpg'\n",
    "    predict_vit(vit_model, img_test_path, list(training.class_indices.keys()))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1569286,
     "sourceId": 2583382,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30120,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
